# ğŸ§  Sign Language to Speech Converter ğŸ”Š

A Python-based real-time sign language to speech converter using **OpenCV**, **MediaPipe**, **scikit-learn**, **pyttsx3**, and **tkinter**. This project enables users to train custom hand gesture classes and convert them into spoken words in real time â€” making communication more inclusive and accessible.

---

## ğŸ“¸ How It Works

1. **Data Collection** â€“ Capture hand gesture images for custom classes via webcam.
2. **Feature Extraction** â€“ Use MediaPipe to extract hand landmarks and normalize them.
3. **Model Training** â€“ Train a Random Forest Classifier on extracted features.
4. **Real-Time Inference** â€“ Predict gesture classes live and convert the output to speech.

---

## ğŸ—ƒï¸ Project Structure

ğŸ“ data/ # Collected gesture images (auto-created) â”œâ”€â”€ 0/ â”œâ”€â”€ 1/ ... ğŸ“„ collect_imgs.py # Collects gesture images from webcam ğŸ“„ create_dataset.py # Extracts features from images and creates dataset ğŸ“„ train_classifier.py # Trains ML model using scikit-learn ğŸ“„ inference.py # Runs live predictions and text-to-speech ğŸ“„ data.pickle # Pickled dataset ğŸ“„ model.p # Pickled trained model


---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/alston06/Sign-language-to-speech
cd Sign-language-to-speech
2. Install Dependencies
pip install opencv-python mediapipe scikit-learn pyttsx3 matplotlib
On Windows, use: pip install pyttsx3 pypiwin32

ğŸ§ª Step-by-Step Usage
âœ… Step 1: Collect Images
python collect_imgs.py
Collects 100 images for 5 gesture classes.

Press Q to begin capture for each class.

âœ… Step 2: Create Dataset
python create_dataset.py
Uses MediaPipe to extract hand landmarks.

Saves data to data.pickle.

âœ… Step 3: Train Classifier
python train_classifier.py
Trains a RandomForest model on the dataset.

Saves model to model.p.

âœ… Step 4: Run Real-time Inference
python inference.py
Select a voice (Male/Female) using the GUI.

Webcam will start and predict signs as spoken words.

ğŸ§  Current Labels
The model currently supports 5 gestures:
labels_dict = {
    0: 'water',
    1: 'money',
    2: 'I love you',
    3: 'Yes',
    4: 'Hello'
}
You can customize these by changing labels_dict in inference.py and updating your dataset.

ğŸŒŸ Features
ğŸ“· Real-time gesture recognition via webcam

ğŸ§  Train your own model with new gesture classes

ğŸ”Š Speech output using pyttsx3

ğŸªŸ Voice selection through a Tkinter GUI

ğŸ“ Easy dataset and model management with Pickle

ğŸ’¡ Future Enhancements
Deep learning integration with CNNs

Support for multi-hand recognition

Multilingual TTS options

Save recognized gestures as full sentences

Web integration for video conferencing

ğŸ¤ Contributing
We welcome all contributions!

To Contribute:
Fork this repo

Create your branch: git checkout -b feature/AmazingFeature

Commit your changes: git commit -m 'Add some feature'

Push to the branch: git push origin feature/AmazingFeature

Open a Pull Request

ğŸ“„ License
This project is licensed under the MIT License.

â­ Show Your Support
If you found this useful, drop a â­ on the repo to show your support!
